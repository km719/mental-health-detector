{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Ns8IcQPJpD"
      },
      "source": [
        "# **Li**nguistic **F**eature **E**xtraction for **S**ystem**a**tic **V**al**e**nce **R**ecognition (LiFESaVeR)\n",
        "CS 6501: Natural Language Processing Final Project\n",
        "\n",
        "Param Damle (psd9vgc), Richard Wang (rxw2cxy), Kabir Menghrajani (km5qte)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6goIu4dPFLr"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "-LvttSsGrHz4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9g7VPHHPCL_",
        "outputId": "ec93261c-3d12-4315-cc9a-1f7d69f49fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snowballstemmer in c:\\python\\python311\\lib\\site-packages (2.2.0)\n",
            "Requirement already satisfied: contractions in c:\\python\\python311\\lib\\site-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in c:\\python\\python311\\lib\\site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in c:\\python\\python311\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in c:\\python\\python311\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "# library imports\n",
        "import random\n",
        "import csv\n",
        "!pip install snowballstemmer\n",
        "import snowballstemmer\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.nn import softmax\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.initializers import RandomNormal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QQKaBhOBipT",
        "outputId": "341bc4ae-4d80-410e-d6a2-3c042bbffa40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\param\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "4YaPxxvcrKRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klz3YUas_TnC"
      },
      "outputs": [],
      "source": [
        "def magnitude(arr, axis=0):\n",
        "  # adjusted L2 norm function that takes norm with modifier to prevent 0 values\n",
        "  return np.linalg.norm(arr, axis=axis) + 1**-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDbYwSPQLKdb"
      },
      "outputs": [],
      "source": [
        "def adv_index(iterable, start, stop):\n",
        "  # advanced indexing adds start and end of sentence tokens to out of bound indexing\n",
        "  return ([\"<bos>\"] * max(0, -start)) + iterable[max(0, start):stop] + ([\"<eos>\"] * (stop - len(iterable)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "VjEu5aIgrNbJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkYvafknR1Jz"
      },
      "source": [
        "We load in our 4 datasets:\n",
        "- **Anxiety**: https://www.kaggle.com/code/docxian/anxiety-and-depression-text-analytics/input\n",
        "\n",
        "  6896 entries of text labeled as 1 to indicate anxiety/depression and 0 to indicate no anxiety/depression. There are 733 entries labeled as 1, and 6247 labeled as 0. The Excel file was converted to CSV beforehand.\n",
        "- **Stress**: https://www.kaggle.com/datasets/kreeshrajani/human-stress-prediction\n",
        "\n",
        "  2343 entries sourced from various mental health subreddits, and is labeled as 0 for no stress and 1 for indicates stress. 21% of the entries come from the r/ptsd subreddit and 19% come from the r/relationships subreddit.\n",
        "- **Depression**: https://www.kaggle.com/datasets/nidhiy07/student-depression-text\n",
        "\n",
        "  7489 entries sourced from various social media platforms, with posts following English grammar from 15-17 yeara old students. The five columns of this dataset are text, labels, age, age category, and gender. The Excel file was converted to CSV beforehand.\n",
        "- **Suicide**: https://www.kaggle.com/datasets/aunanya875/suicidal-tweet-detection-dataset\n",
        "\n",
        "  1778 tweets, with about 37% being potential suicide posts and about 63% being non-suicide posts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8SXpxjDUq64"
      },
      "outputs": [],
      "source": [
        "conditions = ['anxiety', 'stress', 'depression', 'suicide']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae9nYsJ_I11O"
      },
      "outputs": [],
      "source": [
        "max_n_tokenize = 2  # up to this n, we will also tokenize n-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF6EONsCVo2n"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence):\n",
        "  # Turns sentence of form \"sample information\" to [\"sampl\", \"inform\"]\n",
        "  sent = sentence.lower()\n",
        "  sent = contractions.fix(sent)  # to standardize results\n",
        "\n",
        "  sent = ''.join([c for c in sent if c.isalpha() or c.isspace()])\n",
        "  split_sent = sent.split()\n",
        "  stop_word_locations = set([i for i in range(len(split_sent)) if split_sent[i] in stop_words])\n",
        "  stemmer = snowballstemmer.stemmer('english')\n",
        "  stemmed_sent = stemmer.stemWords(split_sent)\n",
        "  tokenized_sentence = []\n",
        "  for n in range(1,max_n_tokenize+1):\n",
        "    # for start in range(1-n,len(stemmed_sent)):\n",
        "    for start in range(len(stemmed_sent)+1-n):\n",
        "      contains_non_stop_word = False\n",
        "      for i in range(start, start + n):\n",
        "        if i not in stop_word_locations:\n",
        "          contains_non_stop_word = True\n",
        "          break\n",
        "      if contains_non_stop_word:\n",
        "        # tokenized_sentence.append(tuple(adv_index(stemmed_sent, start, start + n)))\n",
        "        tokenized_sentence.append(tuple(stemmed_sent[start:start + n]))\n",
        "  return tokenized_sentence\n",
        "\n",
        "def tokenize_corpus(corpus):\n",
        "  ''' input: list of sentences, each sentence is a string\n",
        "      output: list of sentences, each sentence is a list of tokens\n",
        "  '''\n",
        "  return [ tokenize(document) for document in tqdm(corpus) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXYDvZxAMnbW",
        "outputId": "c130c278-5f31-4856-b440-f584979ea061"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('random',),\n",
              " ('string',),\n",
              " ('word',),\n",
              " ('put',),\n",
              " ('togeth',),\n",
              " ('a', 'random'),\n",
              " ('random', 'string'),\n",
              " ('string', 'of'),\n",
              " ('of', 'word'),\n",
              " ('word', 'that'),\n",
              " ('i', 'put'),\n",
              " ('put', 'togeth')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "tokenize(\"here's a random string of words that I put together\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV-YSN4lQ9vo"
      },
      "outputs": [],
      "source": [
        "def load_data(condition, file_name):\n",
        "  # loads data into corpus of documents and corresponding labels\n",
        "  indexes = {\n",
        "      \"anxiety\": (0,1),\n",
        "      \"stress\": (3,4),\n",
        "      \"depression\": (0,1),\n",
        "      \"suicide\": (0,1)\n",
        "  }\n",
        "  text_index, label_index = indexes[condition]\n",
        "\n",
        "  X = []  # list of documents, each document is a raw string\n",
        "  y = []  # list of labels, 0 for condition not present and 1 for condition present\n",
        "\n",
        "  with open(file_name, \"r\", encoding=\"utf8\") as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "      if condition == \"suicide\":  # special dataset format\n",
        "        row[label_index] = '0' if (row[label_index][0] == 'N') else '1'\n",
        "      if row[label_index] in ('0','1'):\n",
        "        X.append(row[text_index])\n",
        "        y.append(int(row[label_index]))\n",
        "  return (X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOxb4nt4RAiC",
        "outputId": "00f83b3e-a251-4f18-e722-539b90d0a824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading anxiety dataset...\n",
            "Loading stress dataset...\n",
            "Loading depression dataset...\n",
            "Loading suicide dataset...\n"
          ]
        }
      ],
      "source": [
        "condition_data = {}  # maps condition name to (corpus of documents, class labels)\n",
        "for condition in conditions:\n",
        "  print(\"Loading\", condition, \"dataset...\")\n",
        "  condition_data[condition] = load_data(condition, condition + '.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0acIfJlS2LW"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzS_9BGaBHMa"
      },
      "outputs": [],
      "source": [
        "def Classifier(num_docs):\n",
        "    # Just the class that will take the similarity scores for each document in the corpus and output a prediction\n",
        "    # Create model\n",
        "    initializer = RandomNormal(mean=0.0, stddev=2.0, seed=420)\n",
        "    model = Sequential()\n",
        "    model.add(keras.Input(shape=(num_docs,)))\n",
        "    model.add(Dense(256, activation='linear', kernel_initializer=initializer))\n",
        "    # model.add(Dense(128, activation='relu', kernel_initializer=initializer))\n",
        "    model.add(Dropout(0.1))\n",
        "    # model.add(Dense(32, activation='relu', kernel_initializer=initializer))\n",
        "    model.add(Dense(1, activation='sigmoid', kernel_initializer=initializer))\n",
        "\n",
        "    # Compile model\n",
        "    # model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), optimizer='nadam', metrics=[\"accuracy\", keras.metrics.FBetaScore(beta=1.75)])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUwwwUDaBHMa"
      },
      "outputs": [],
      "source": [
        "class ThresholdCallback(tf.keras.callbacks.Callback):\n",
        "    # a class that will stop model training once it's sufficiently trained\n",
        "    def __init__(self, metric, min_metric, min_epochs):\n",
        "        # terminates training when accuracy and epochs are both above threshold\n",
        "        super(ThresholdCallback, self).__init__()\n",
        "        self.metric = metric\n",
        "        self.metric_threshold = min_metric\n",
        "        self.epoch_threshold = min_epochs\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        metric_level = logs[self.metric]\n",
        "        if metric_level >= self.metric_threshold and (epoch+1) >= self.epoch_threshold:\n",
        "            self.model.stop_training = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hsw21rEvod5y"
      },
      "outputs": [],
      "source": [
        "# Build TF-IDF Vectorizer given tokenized data\n",
        "\n",
        "# Class for training, builds vocab mapping (unique words) as well as TFIDF matrix (# of words in vocab *  # sentences)\n",
        "class DetectionModel:\n",
        "\n",
        "    def format_tf(self, raw_tf_vec):\n",
        "        # transforms raw occurrence count to logarithmic tf vector\n",
        "        return [ (1 + np.log10(tf) if tf > 0 else 0) for tf in raw_tf_vec ]\n",
        "\n",
        "    def train(self, corpus, labels):\n",
        "        ''' corpus should be formatted as list of documents, where each document is a string\n",
        "            labels should be a list of corresponding integer class labels\n",
        "        '''\n",
        "        # Shuffle dataset for robustness\n",
        "        print(\"Shuffling training data...\")\n",
        "        training_data = list(zip(corpus, labels))\n",
        "        random.shuffle(training_data)\n",
        "        corpus, labels = zip(*training_data)\n",
        "\n",
        "        # Create vocab from corpus\n",
        "        print(\"Tokenizing corpus...\")\n",
        "        tokenized_corpus = tokenize_corpus(corpus)\n",
        "        self.num_documents = len(tokenized_corpus)\n",
        "        self.pos_y = np.asarray(labels,dtype=bool)\n",
        "        self.neg_y = np.invert(self.pos_y)\n",
        "\n",
        "        self.num_pos = np.sum(self.pos_y)\n",
        "        self.num_neg = self.num_documents - self.num_pos\n",
        "        # print(self.num_documents, self.num_pos, self.num_neg)\n",
        "\n",
        "        self.doc_weights = np.ones(self.num_documents) # used in case we want to assign documents different weights\n",
        "        self.vocab = {}  # maps token to id\n",
        "\n",
        "        id = 0\n",
        "        for sentence in tokenized_corpus:\n",
        "            for token in sentence:\n",
        "                if token not in self.vocab:\n",
        "                    self.vocab[token] = id\n",
        "                    id += 1\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "        print(\"Calculating term frequencies...\")\n",
        "        tf_matrix = []\n",
        "        for sentence in tqdm(tokenized_corpus):\n",
        "            sentence_tf = [0] * self.vocab_size\n",
        "            for token in sentence:\n",
        "                sentence_tf[self.vocab[token]] += 1\n",
        "            tf_matrix.append(self.format_tf(sentence_tf))\n",
        "\n",
        "        print(\"Calculating inverted document frequencies...\")\n",
        "        token_df = [0] * self.vocab_size\n",
        "        for document in tokenized_corpus:\n",
        "            token_ids_in_document = set()\n",
        "            for token in document:\n",
        "                token_ids_in_document.add(self.vocab[token])\n",
        "            for id in token_ids_in_document:\n",
        "                token_df[id] += 1\n",
        "        token_idf = [np.log10(self.num_documents / df) for df in token_df]\n",
        "\n",
        "        print(\"Calculating tf-idf scores...\")\n",
        "        self.tfidf = np.multiply(tf_matrix, token_idf)\n",
        "        self.tfidf /= magnitude(self.tfidf, axis=1)[:,np.newaxis]  # normalization so that dot product is cosine similarity\n",
        "        self.tfidf = self.tfidf.T  # transposed so we can do row_sample*tfidf to get a row of outputs\n",
        "\n",
        "        ''' instead of simply averaging the similarity scores across documents,\n",
        "            we develop a neural net that converts document similarity into a binary prediction\n",
        "        '''\n",
        "        print(\"Training neural network...\")\n",
        "        scaled_tf_matrix = np.asarray(tf_matrix, dtype=np.float64)\n",
        "        scaled_tf_matrix /= magnitude(scaled_tf_matrix, axis=1)[:,np.newaxis]\n",
        "        self.classifier = Classifier(self.num_documents)\n",
        "        train_X = np.matmul(scaled_tf_matrix, self.tfidf)  # result will have a row per document tf vector, where each column is its similarity score with every document's tfidf vector\n",
        "        train_y = np.array(self.pos_y, dtype=np.float64)\n",
        "        hist = self.classifier.fit(train_X, train_y, epochs=100, batch_size=32, shuffle=True, validation_split=0.15, callbacks=[ThresholdCallback(metric = \"fbeta_score\", min_metric = 0.97, min_epochs = 10)])\n",
        "        return hist\n",
        "\n",
        "\n",
        "    def update_doc_weights(self, new_doc_weights):\n",
        "      # in case different documents should be weighted differently\n",
        "      self.doc_weights = np.array(new_doc_weights, dtype=np.float64) + np.min(new_doc_weights)\n",
        "      # scale so that the magnitudes across positive and negative samples add up to the number of positive and negative samples, respectively\n",
        "      for (indexes, instances) in ((self.pos_y, self.num_pos), (self.neg_y, self.num_neg)):\n",
        "        sum_weights = np.sum(self.doc_weights[indexes])\n",
        "        if sum_weights == 0: # protect division by 0\n",
        "          sum_weights = 1\n",
        "        self.doc_weights[indexes] *= instances / sum_weights\n",
        "\n",
        "\n",
        "    def score(self, sentence):\n",
        "        tokenized_sentence = tokenize(sentence)\n",
        "\n",
        "        token_ids = []\n",
        "        for token in tokenized_sentence:\n",
        "            if token in self.vocab:\n",
        "                token_ids.append(self.vocab[token])\n",
        "\n",
        "        sentence_tf_raw = [0] * self.vocab_size\n",
        "        for id in token_ids:\n",
        "            sentence_tf_raw[id] += 1\n",
        "        sentence_tf = np.asarray(self.format_tf(sentence_tf_raw), dtype=np.float64)\n",
        "        sentence_magnitude = magnitude(sentence_tf)\n",
        "        if sentence_magnitude == 0: # protect division by 0\n",
        "          sentence_magnitude = 1\n",
        "        sentence_tf /= sentence_magnitude  # normalization so that dot product is cosine similarity\n",
        "\n",
        "        # cosine of angle between sentence given and document from training corpus\n",
        "        similarity_per_doc = np.matmul(sentence_tf, self.tfidf)\n",
        "\n",
        "        return self.classifier.predict(similarity_per_doc[np.newaxis,:], verbose=0).item()\n",
        "\n",
        "        # # if we consider certain documents from training corpus to be more important\n",
        "        # weighted_similarity = np.multiply(similarity_per_doc, self.doc_weights)\n",
        "\n",
        "        # ''' since we are subtracting off a negative similarity score, we are effectively eliminating\n",
        "        #     the \"noise\" and we can scale our similarity score so that both similarity values\n",
        "        #     are not hovering around 0.05\n",
        "        # '''\n",
        "        # similarity_scaling_factor = np.max(weighted_similarity)\n",
        "        # if similarity_scaling_factor == 0: # protect division by 0\n",
        "        #   similarity_scaling_factor = 1\n",
        "\n",
        "        # # find separate scores for our positive and negative samples\n",
        "        # positive_score = np.sum(weighted_similarity[self.pos_y]) / (self.num_pos * similarity_scaling_factor)\n",
        "        # negative_score = np.sum(weighted_similarity[self.neg_y]) / (self.num_neg * similarity_scaling_factor)\n",
        "\n",
        "        # scaled_similarity = (positive_score - negative_score + 1.0)/2.0  # converts similarity in [-1,1] to [0,1]\n",
        "\n",
        "        return scaled_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oIJ1olpRsmU",
        "outputId": "3fee127b-992c-426a-8172-853f8ad59092",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building anxiety detector...\n",
            "Shuffling training data...\n",
            "Tokenizing corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████| 6980/6980 [00:04<00:00, 1429.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating term frequencies...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 6980/6980 [00:11<00:00, 631.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating inverted document frequencies...\n",
            "Calculating tf-idf scores...\n",
            "Training neural network...\n",
            "Epoch 1/100\n",
            "186/186 [==============================] - 5s 22ms/step - loss: 8.9471 - accuracy: 0.8033 - fbeta_score: 0.4526 - val_loss: 2.7858 - val_accuracy: 0.9217 - val_fbeta_score: 0.6966\n",
            "Epoch 2/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 2.8905 - accuracy: 0.9302 - fbeta_score: 0.6952 - val_loss: 1.7950 - val_accuracy: 0.9475 - val_fbeta_score: 0.7953\n",
            "Epoch 3/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 2.0429 - accuracy: 0.9493 - fbeta_score: 0.7697 - val_loss: 1.4165 - val_accuracy: 0.9666 - val_fbeta_score: 0.8562\n",
            "Epoch 4/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 1.6296 - accuracy: 0.9629 - fbeta_score: 0.8198 - val_loss: 1.2601 - val_accuracy: 0.9761 - val_fbeta_score: 0.8852\n",
            "Epoch 5/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 1.2254 - accuracy: 0.9675 - fbeta_score: 0.8354 - val_loss: 1.1892 - val_accuracy: 0.9809 - val_fbeta_score: 0.9024\n",
            "Epoch 6/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 1.0247 - accuracy: 0.9757 - fbeta_score: 0.8709 - val_loss: 1.2594 - val_accuracy: 0.9733 - val_fbeta_score: 0.8995\n",
            "Epoch 7/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.8602 - accuracy: 0.9784 - fbeta_score: 0.8839 - val_loss: 1.2045 - val_accuracy: 0.9799 - val_fbeta_score: 0.8985\n",
            "Epoch 8/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.7640 - accuracy: 0.9796 - fbeta_score: 0.9002 - val_loss: 1.2514 - val_accuracy: 0.9809 - val_fbeta_score: 0.8975\n",
            "Epoch 9/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.7854 - accuracy: 0.9771 - fbeta_score: 0.8809 - val_loss: 1.1312 - val_accuracy: 0.9828 - val_fbeta_score: 0.9122\n",
            "Epoch 10/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.6768 - accuracy: 0.9816 - fbeta_score: 0.9048 - val_loss: 1.1407 - val_accuracy: 0.9828 - val_fbeta_score: 0.9093\n",
            "Epoch 11/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.6047 - accuracy: 0.9830 - fbeta_score: 0.9116 - val_loss: 1.3431 - val_accuracy: 0.9771 - val_fbeta_score: 0.9092\n",
            "Epoch 12/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.5193 - accuracy: 0.9847 - fbeta_score: 0.9138 - val_loss: 1.2948 - val_accuracy: 0.9828 - val_fbeta_score: 0.9073\n",
            "Epoch 13/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.4507 - accuracy: 0.9862 - fbeta_score: 0.9173 - val_loss: 1.2800 - val_accuracy: 0.9838 - val_fbeta_score: 0.8937\n",
            "Epoch 14/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.4430 - accuracy: 0.9860 - fbeta_score: 0.9204 - val_loss: 1.2025 - val_accuracy: 0.9847 - val_fbeta_score: 0.9054\n",
            "Epoch 15/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.4120 - accuracy: 0.9865 - fbeta_score: 0.9372 - val_loss: 1.5102 - val_accuracy: 0.9790 - val_fbeta_score: 0.9131\n",
            "Epoch 16/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.3727 - accuracy: 0.9863 - fbeta_score: 0.9325 - val_loss: 1.2867 - val_accuracy: 0.9828 - val_fbeta_score: 0.9174\n",
            "Epoch 17/100\n",
            "186/186 [==============================] - 4s 22ms/step - loss: 0.3605 - accuracy: 0.9877 - fbeta_score: 0.9442 - val_loss: 1.5068 - val_accuracy: 0.9790 - val_fbeta_score: 0.9191\n",
            "Epoch 18/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.2870 - accuracy: 0.9909 - fbeta_score: 0.9480 - val_loss: 1.4012 - val_accuracy: 0.9799 - val_fbeta_score: 0.9133\n",
            "Epoch 19/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.3171 - accuracy: 0.9911 - fbeta_score: 0.9420 - val_loss: 1.3670 - val_accuracy: 0.9799 - val_fbeta_score: 0.9122\n",
            "Epoch 20/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.2948 - accuracy: 0.9884 - fbeta_score: 0.9383 - val_loss: 1.4157 - val_accuracy: 0.9799 - val_fbeta_score: 0.9223\n",
            "Epoch 21/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.2531 - accuracy: 0.9902 - fbeta_score: 0.9431 - val_loss: 1.4132 - val_accuracy: 0.9809 - val_fbeta_score: 0.9054\n",
            "Epoch 22/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.2451 - accuracy: 0.9912 - fbeta_score: 0.9483 - val_loss: 1.2938 - val_accuracy: 0.9809 - val_fbeta_score: 0.9231\n",
            "Epoch 23/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.2220 - accuracy: 0.9911 - fbeta_score: 0.9535 - val_loss: 1.4042 - val_accuracy: 0.9819 - val_fbeta_score: 0.9131\n",
            "Epoch 24/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.1822 - accuracy: 0.9909 - fbeta_score: 0.9514 - val_loss: 1.4579 - val_accuracy: 0.9828 - val_fbeta_score: 0.9131\n",
            "Epoch 25/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.2072 - accuracy: 0.9909 - fbeta_score: 0.9530 - val_loss: 1.4180 - val_accuracy: 0.9828 - val_fbeta_score: 0.9313\n",
            "Epoch 26/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.1351 - accuracy: 0.9938 - fbeta_score: 0.9581 - val_loss: 1.3641 - val_accuracy: 0.9838 - val_fbeta_score: 0.9272\n",
            "Epoch 27/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.1777 - accuracy: 0.9921 - fbeta_score: 0.9496 - val_loss: 1.3966 - val_accuracy: 0.9809 - val_fbeta_score: 0.9151\n",
            "Epoch 28/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.1624 - accuracy: 0.9934 - fbeta_score: 0.9527 - val_loss: 1.4578 - val_accuracy: 0.9799 - val_fbeta_score: 0.9231\n",
            "Epoch 29/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.1604 - accuracy: 0.9934 - fbeta_score: 0.9610 - val_loss: 1.3387 - val_accuracy: 0.9799 - val_fbeta_score: 0.9211\n",
            "Epoch 30/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.1480 - accuracy: 0.9929 - fbeta_score: 0.9661 - val_loss: 1.4356 - val_accuracy: 0.9809 - val_fbeta_score: 0.9231\n",
            "Epoch 31/100\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.2019 - accuracy: 0.9912 - fbeta_score: 0.9556 - val_loss: 1.3703 - val_accuracy: 0.9790 - val_fbeta_score: 0.9313\n",
            "Epoch 32/100\n",
            "186/186 [==============================] - 4s 22ms/step - loss: 0.1058 - accuracy: 0.9949 - fbeta_score: 0.9716 - val_loss: 1.5209 - val_accuracy: 0.9828 - val_fbeta_score: 0.9333\n",
            "\n",
            "Building stress detector...\n",
            "Shuffling training data...\n",
            "Tokenizing corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 2838/2838 [00:08<00:00, 344.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating term frequencies...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 2838/2838 [00:08<00:00, 328.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating inverted document frequencies...\n",
            "Calculating tf-idf scores...\n",
            "Training neural network...\n",
            "Epoch 1/100\n",
            "76/76 [==============================] - 2s 11ms/step - loss: 23.7703 - accuracy: 0.5473 - fbeta_score: 0.6643 - val_loss: 20.1225 - val_accuracy: 0.5563 - val_fbeta_score: 0.6481\n",
            "Epoch 2/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 19.2760 - accuracy: 0.5995 - fbeta_score: 0.7042 - val_loss: 17.4173 - val_accuracy: 0.6009 - val_fbeta_score: 0.6845\n",
            "Epoch 3/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 16.8979 - accuracy: 0.6430 - fbeta_score: 0.7346 - val_loss: 16.3551 - val_accuracy: 0.6174 - val_fbeta_score: 0.6617\n",
            "Epoch 4/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 14.7945 - accuracy: 0.6654 - fbeta_score: 0.7519 - val_loss: 14.8676 - val_accuracy: 0.6573 - val_fbeta_score: 0.7361\n",
            "Epoch 5/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 14.0694 - accuracy: 0.6762 - fbeta_score: 0.7625 - val_loss: 14.2983 - val_accuracy: 0.6596 - val_fbeta_score: 0.7238\n",
            "Epoch 6/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 12.7643 - accuracy: 0.6920 - fbeta_score: 0.7848 - val_loss: 14.2564 - val_accuracy: 0.6643 - val_fbeta_score: 0.6899\n",
            "Epoch 7/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 11.3548 - accuracy: 0.7027 - fbeta_score: 0.7791 - val_loss: 13.4060 - val_accuracy: 0.6784 - val_fbeta_score: 0.7514\n",
            "Epoch 8/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 10.7628 - accuracy: 0.7143 - fbeta_score: 0.7931 - val_loss: 13.0690 - val_accuracy: 0.6761 - val_fbeta_score: 0.7592\n",
            "Epoch 9/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 9.9150 - accuracy: 0.7247 - fbeta_score: 0.7977 - val_loss: 13.2935 - val_accuracy: 0.6972 - val_fbeta_score: 0.7934\n",
            "Epoch 10/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 9.3952 - accuracy: 0.7355 - fbeta_score: 0.8151 - val_loss: 12.7817 - val_accuracy: 0.6831 - val_fbeta_score: 0.7691\n",
            "Epoch 11/100\n",
            "76/76 [==============================] - 1s 11ms/step - loss: 8.8480 - accuracy: 0.7463 - fbeta_score: 0.8181 - val_loss: 12.7769 - val_accuracy: 0.6878 - val_fbeta_score: 0.7380\n",
            "Epoch 12/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 8.1879 - accuracy: 0.7529 - fbeta_score: 0.8276 - val_loss: 12.3873 - val_accuracy: 0.6878 - val_fbeta_score: 0.7617\n",
            "Epoch 13/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 7.9576 - accuracy: 0.7620 - fbeta_score: 0.8259 - val_loss: 12.3523 - val_accuracy: 0.6854 - val_fbeta_score: 0.7716\n",
            "Epoch 14/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 6.8623 - accuracy: 0.7707 - fbeta_score: 0.8435 - val_loss: 12.2794 - val_accuracy: 0.7019 - val_fbeta_score: 0.7716\n",
            "Epoch 15/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 6.5595 - accuracy: 0.7682 - fbeta_score: 0.8425 - val_loss: 12.2323 - val_accuracy: 0.6901 - val_fbeta_score: 0.7504\n",
            "Epoch 16/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 6.0985 - accuracy: 0.7832 - fbeta_score: 0.8493 - val_loss: 12.0951 - val_accuracy: 0.7066 - val_fbeta_score: 0.7872\n",
            "Epoch 17/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 5.8957 - accuracy: 0.7832 - fbeta_score: 0.8575 - val_loss: 12.0241 - val_accuracy: 0.6901 - val_fbeta_score: 0.7535\n",
            "Epoch 18/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 5.4386 - accuracy: 0.8002 - fbeta_score: 0.8628 - val_loss: 11.9262 - val_accuracy: 0.6948 - val_fbeta_score: 0.7750\n",
            "Epoch 19/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 4.7124 - accuracy: 0.8138 - fbeta_score: 0.8713 - val_loss: 12.3935 - val_accuracy: 0.6854 - val_fbeta_score: 0.7279\n",
            "Epoch 20/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 4.8856 - accuracy: 0.8105 - fbeta_score: 0.8684 - val_loss: 12.1022 - val_accuracy: 0.6995 - val_fbeta_score: 0.7420\n",
            "Epoch 21/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 4.9618 - accuracy: 0.8006 - fbeta_score: 0.8646 - val_loss: 11.7589 - val_accuracy: 0.6995 - val_fbeta_score: 0.7578\n",
            "Epoch 22/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 4.0251 - accuracy: 0.8259 - fbeta_score: 0.8832 - val_loss: 11.8309 - val_accuracy: 0.7042 - val_fbeta_score: 0.7923\n",
            "Epoch 23/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 3.8334 - accuracy: 0.8342 - fbeta_score: 0.8885 - val_loss: 11.8435 - val_accuracy: 0.7113 - val_fbeta_score: 0.7957\n",
            "Epoch 24/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 3.8222 - accuracy: 0.8387 - fbeta_score: 0.8875 - val_loss: 11.9680 - val_accuracy: 0.7136 - val_fbeta_score: 0.8178\n",
            "Epoch 25/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 3.6917 - accuracy: 0.8342 - fbeta_score: 0.8823 - val_loss: 11.6474 - val_accuracy: 0.7089 - val_fbeta_score: 0.7870\n",
            "Epoch 26/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 3.4859 - accuracy: 0.8499 - fbeta_score: 0.8891 - val_loss: 11.6488 - val_accuracy: 0.7160 - val_fbeta_score: 0.7589\n",
            "Epoch 27/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 3.0836 - accuracy: 0.8458 - fbeta_score: 0.9003 - val_loss: 11.5508 - val_accuracy: 0.7136 - val_fbeta_score: 0.7818\n",
            "Epoch 28/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 2.7690 - accuracy: 0.8706 - fbeta_score: 0.9012 - val_loss: 11.6845 - val_accuracy: 0.7113 - val_fbeta_score: 0.7527\n",
            "Epoch 29/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 2.6781 - accuracy: 0.8586 - fbeta_score: 0.9026 - val_loss: 11.7086 - val_accuracy: 0.7019 - val_fbeta_score: 0.7895\n",
            "Epoch 30/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 2.6620 - accuracy: 0.8628 - fbeta_score: 0.9066 - val_loss: 11.6628 - val_accuracy: 0.7183 - val_fbeta_score: 0.7687\n",
            "Epoch 31/100\n",
            "76/76 [==============================] - 1s 11ms/step - loss: 2.3375 - accuracy: 0.8669 - fbeta_score: 0.9081 - val_loss: 11.8160 - val_accuracy: 0.7089 - val_fbeta_score: 0.7578\n",
            "Epoch 32/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 2.3180 - accuracy: 0.8715 - fbeta_score: 0.9135 - val_loss: 11.7645 - val_accuracy: 0.6995 - val_fbeta_score: 0.8008\n",
            "Epoch 33/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 2.3324 - accuracy: 0.8727 - fbeta_score: 0.9075 - val_loss: 11.7772 - val_accuracy: 0.6995 - val_fbeta_score: 0.7846\n",
            "Epoch 34/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.8711 - accuracy: 0.8901 - fbeta_score: 0.9211 - val_loss: 11.8897 - val_accuracy: 0.7136 - val_fbeta_score: 0.7603\n",
            "Epoch 35/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.9018 - accuracy: 0.8897 - fbeta_score: 0.9138 - val_loss: 11.9712 - val_accuracy: 0.7089 - val_fbeta_score: 0.8119\n",
            "Epoch 36/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.7785 - accuracy: 0.8852 - fbeta_score: 0.9214 - val_loss: 11.7312 - val_accuracy: 0.7113 - val_fbeta_score: 0.7598\n",
            "Epoch 37/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.7704 - accuracy: 0.8964 - fbeta_score: 0.9266 - val_loss: 11.6465 - val_accuracy: 0.7089 - val_fbeta_score: 0.7757\n",
            "Epoch 38/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.4821 - accuracy: 0.9059 - fbeta_score: 0.9256 - val_loss: 11.6898 - val_accuracy: 0.7136 - val_fbeta_score: 0.7897\n",
            "Epoch 39/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.6202 - accuracy: 0.8914 - fbeta_score: 0.9247 - val_loss: 11.6952 - val_accuracy: 0.7183 - val_fbeta_score: 0.7791\n",
            "Epoch 40/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.4387 - accuracy: 0.9084 - fbeta_score: 0.9331 - val_loss: 11.7213 - val_accuracy: 0.7160 - val_fbeta_score: 0.7791\n",
            "Epoch 41/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.3986 - accuracy: 0.9133 - fbeta_score: 0.9325 - val_loss: 11.6416 - val_accuracy: 0.7136 - val_fbeta_score: 0.7720\n",
            "Epoch 42/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.1729 - accuracy: 0.9158 - fbeta_score: 0.9306 - val_loss: 11.7757 - val_accuracy: 0.7136 - val_fbeta_score: 0.7630\n",
            "Epoch 43/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.0824 - accuracy: 0.9196 - fbeta_score: 0.9346 - val_loss: 11.8333 - val_accuracy: 0.7113 - val_fbeta_score: 0.7722\n",
            "Epoch 44/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.1952 - accuracy: 0.9216 - fbeta_score: 0.9349 - val_loss: 11.8944 - val_accuracy: 0.7160 - val_fbeta_score: 0.7532\n",
            "Epoch 45/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.0073 - accuracy: 0.9221 - fbeta_score: 0.9384 - val_loss: 11.9151 - val_accuracy: 0.7207 - val_fbeta_score: 0.7567\n",
            "Epoch 46/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.0250 - accuracy: 0.9229 - fbeta_score: 0.9394 - val_loss: 11.9127 - val_accuracy: 0.7113 - val_fbeta_score: 0.7608\n",
            "Epoch 47/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.0505 - accuracy: 0.9283 - fbeta_score: 0.9410 - val_loss: 11.8474 - val_accuracy: 0.7254 - val_fbeta_score: 0.7507\n",
            "Epoch 48/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 1.0061 - accuracy: 0.9250 - fbeta_score: 0.9371 - val_loss: 12.0573 - val_accuracy: 0.7136 - val_fbeta_score: 0.8051\n",
            "Epoch 49/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.9574 - accuracy: 0.9312 - fbeta_score: 0.9410 - val_loss: 12.1394 - val_accuracy: 0.7136 - val_fbeta_score: 0.7437\n",
            "Epoch 50/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.8485 - accuracy: 0.9299 - fbeta_score: 0.9440 - val_loss: 11.8399 - val_accuracy: 0.7254 - val_fbeta_score: 0.7746\n",
            "Epoch 51/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.8160 - accuracy: 0.9382 - fbeta_score: 0.9467 - val_loss: 11.7388 - val_accuracy: 0.7230 - val_fbeta_score: 0.7783\n",
            "Epoch 52/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.8513 - accuracy: 0.9320 - fbeta_score: 0.9448 - val_loss: 12.0723 - val_accuracy: 0.7113 - val_fbeta_score: 0.7489\n",
            "Epoch 53/100\n",
            "76/76 [==============================] - 1s 11ms/step - loss: 0.7786 - accuracy: 0.9337 - fbeta_score: 0.9471 - val_loss: 12.0289 - val_accuracy: 0.7160 - val_fbeta_score: 0.7556\n",
            "Epoch 54/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.6517 - accuracy: 0.9436 - fbeta_score: 0.9565 - val_loss: 11.8384 - val_accuracy: 0.7136 - val_fbeta_score: 0.7592\n",
            "Epoch 55/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.7070 - accuracy: 0.9420 - fbeta_score: 0.9526 - val_loss: 11.8612 - val_accuracy: 0.7136 - val_fbeta_score: 0.7525\n",
            "Epoch 56/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.7171 - accuracy: 0.9395 - fbeta_score: 0.9508 - val_loss: 11.7867 - val_accuracy: 0.7183 - val_fbeta_score: 0.7703\n",
            "Epoch 57/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.6436 - accuracy: 0.9461 - fbeta_score: 0.9517 - val_loss: 11.8215 - val_accuracy: 0.7207 - val_fbeta_score: 0.7647\n",
            "Epoch 58/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.6656 - accuracy: 0.9453 - fbeta_score: 0.9504 - val_loss: 11.8707 - val_accuracy: 0.7254 - val_fbeta_score: 0.7703\n",
            "Epoch 59/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.5701 - accuracy: 0.9511 - fbeta_score: 0.9513 - val_loss: 11.8373 - val_accuracy: 0.7277 - val_fbeta_score: 0.7625\n",
            "Epoch 60/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.5563 - accuracy: 0.9519 - fbeta_score: 0.9555 - val_loss: 11.8366 - val_accuracy: 0.7300 - val_fbeta_score: 0.7730\n",
            "Epoch 61/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.6197 - accuracy: 0.9498 - fbeta_score: 0.9583 - val_loss: 12.0171 - val_accuracy: 0.7254 - val_fbeta_score: 0.7939\n",
            "Epoch 62/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.4938 - accuracy: 0.9565 - fbeta_score: 0.9595 - val_loss: 12.0181 - val_accuracy: 0.7254 - val_fbeta_score: 0.7931\n",
            "Epoch 63/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.4109 - accuracy: 0.9594 - fbeta_score: 0.9612 - val_loss: 12.1444 - val_accuracy: 0.7089 - val_fbeta_score: 0.7633\n",
            "Epoch 64/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.4058 - accuracy: 0.9598 - fbeta_score: 0.9611 - val_loss: 12.0876 - val_accuracy: 0.7207 - val_fbeta_score: 0.7655\n",
            "Epoch 65/100\n",
            "76/76 [==============================] - 1s 9ms/step - loss: 0.4142 - accuracy: 0.9594 - fbeta_score: 0.9619 - val_loss: 11.9441 - val_accuracy: 0.7160 - val_fbeta_score: 0.7816\n",
            "Epoch 66/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.6058 - accuracy: 0.9548 - fbeta_score: 0.9551 - val_loss: 12.0267 - val_accuracy: 0.7254 - val_fbeta_score: 0.7728\n",
            "Epoch 67/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.4426 - accuracy: 0.9594 - fbeta_score: 0.9626 - val_loss: 11.9846 - val_accuracy: 0.7207 - val_fbeta_score: 0.7746\n",
            "Epoch 68/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.3606 - accuracy: 0.9643 - fbeta_score: 0.9594 - val_loss: 12.2079 - val_accuracy: 0.7113 - val_fbeta_score: 0.7520\n",
            "Epoch 69/100\n",
            "76/76 [==============================] - 1s 9ms/step - loss: 0.3831 - accuracy: 0.9664 - fbeta_score: 0.9638 - val_loss: 12.2054 - val_accuracy: 0.7136 - val_fbeta_score: 0.7584\n",
            "Epoch 70/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.3384 - accuracy: 0.9664 - fbeta_score: 0.9616 - val_loss: 12.0879 - val_accuracy: 0.7254 - val_fbeta_score: 0.7578\n",
            "Epoch 71/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.5009 - accuracy: 0.9548 - fbeta_score: 0.9552 - val_loss: 12.0578 - val_accuracy: 0.7183 - val_fbeta_score: 0.7818\n",
            "Epoch 72/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.3862 - accuracy: 0.9660 - fbeta_score: 0.9627 - val_loss: 12.2375 - val_accuracy: 0.7254 - val_fbeta_score: 0.7630\n",
            "Epoch 73/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.3494 - accuracy: 0.9668 - fbeta_score: 0.9659 - val_loss: 12.1921 - val_accuracy: 0.7254 - val_fbeta_score: 0.7999\n",
            "Epoch 74/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.4251 - accuracy: 0.9652 - fbeta_score: 0.9642 - val_loss: 11.9954 - val_accuracy: 0.7324 - val_fbeta_score: 0.7823\n",
            "Epoch 75/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.2627 - accuracy: 0.9701 - fbeta_score: 0.9658 - val_loss: 11.9302 - val_accuracy: 0.7300 - val_fbeta_score: 0.7876\n",
            "Epoch 76/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.3503 - accuracy: 0.9701 - fbeta_score: 0.9631 - val_loss: 12.0406 - val_accuracy: 0.7254 - val_fbeta_score: 0.7682\n",
            "Epoch 77/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.3852 - accuracy: 0.9701 - fbeta_score: 0.9681 - val_loss: 11.9760 - val_accuracy: 0.7230 - val_fbeta_score: 0.7869\n",
            "Epoch 78/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.3450 - accuracy: 0.9714 - fbeta_score: 0.9645 - val_loss: 11.9279 - val_accuracy: 0.7230 - val_fbeta_score: 0.7853\n",
            "Epoch 79/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.2708 - accuracy: 0.9722 - fbeta_score: 0.9666 - val_loss: 12.1139 - val_accuracy: 0.7254 - val_fbeta_score: 0.7584\n",
            "Epoch 80/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.2599 - accuracy: 0.9681 - fbeta_score: 0.9693 - val_loss: 12.0869 - val_accuracy: 0.7207 - val_fbeta_score: 0.7728\n",
            "Epoch 81/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.3776 - accuracy: 0.9710 - fbeta_score: 0.9675 - val_loss: 11.9430 - val_accuracy: 0.7207 - val_fbeta_score: 0.7929\n",
            "Epoch 82/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.3607 - accuracy: 0.9681 - fbeta_score: 0.9684 - val_loss: 12.1267 - val_accuracy: 0.7136 - val_fbeta_score: 0.7990\n",
            "Epoch 83/100\n",
            "76/76 [==============================] - 1s 11ms/step - loss: 0.2253 - accuracy: 0.9780 - fbeta_score: 0.9696 - val_loss: 12.2511 - val_accuracy: 0.7160 - val_fbeta_score: 0.8076\n",
            "Epoch 84/100\n",
            "76/76 [==============================] - 1s 10ms/step - loss: 0.2292 - accuracy: 0.9793 - fbeta_score: 0.9709 - val_loss: 12.2587 - val_accuracy: 0.7136 - val_fbeta_score: 0.7808\n",
            "\n",
            "Building depression detector...\n",
            "Shuffling training data...\n",
            "Tokenizing corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████| 7486/7486 [00:03<00:00, 2181.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating term frequencies...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 7486/7486 [00:12<00:00, 592.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating inverted document frequencies...\n",
            "Calculating tf-idf scores...\n",
            "Training neural network...\n",
            "Epoch 1/100\n",
            "199/199 [==============================] - 6s 24ms/step - loss: 10.2533 - accuracy: 0.7682 - fbeta_score: 0.5288 - val_loss: 5.6827 - val_accuracy: 0.8718 - val_fbeta_score: 0.6380\n",
            "Epoch 2/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 4.7137 - accuracy: 0.8834 - fbeta_score: 0.7000 - val_loss: 4.1318 - val_accuracy: 0.9038 - val_fbeta_score: 0.7173\n",
            "Epoch 3/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 3.2608 - accuracy: 0.9112 - fbeta_score: 0.7659 - val_loss: 3.7557 - val_accuracy: 0.9154 - val_fbeta_score: 0.7523\n",
            "Epoch 4/100\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 2.7151 - accuracy: 0.9279 - fbeta_score: 0.7917 - val_loss: 3.2696 - val_accuracy: 0.9216 - val_fbeta_score: 0.7636\n",
            "Epoch 5/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 2.1218 - accuracy: 0.9422 - fbeta_score: 0.8250 - val_loss: 3.0800 - val_accuracy: 0.9252 - val_fbeta_score: 0.7867\n",
            "Epoch 6/100\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 2.0741 - accuracy: 0.9423 - fbeta_score: 0.8376 - val_loss: 3.2349 - val_accuracy: 0.9225 - val_fbeta_score: 0.7816\n",
            "Epoch 7/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 1.9140 - accuracy: 0.9434 - fbeta_score: 0.8358 - val_loss: 3.0095 - val_accuracy: 0.9261 - val_fbeta_score: 0.7844\n",
            "Epoch 8/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 1.6751 - accuracy: 0.9500 - fbeta_score: 0.8524 - val_loss: 2.9925 - val_accuracy: 0.9305 - val_fbeta_score: 0.7795\n",
            "Epoch 9/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 1.3778 - accuracy: 0.9569 - fbeta_score: 0.8684 - val_loss: 3.0267 - val_accuracy: 0.9297 - val_fbeta_score: 0.7901\n",
            "Epoch 10/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 1.3460 - accuracy: 0.9569 - fbeta_score: 0.8710 - val_loss: 2.9920 - val_accuracy: 0.9270 - val_fbeta_score: 0.8044\n",
            "Epoch 11/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 1.3806 - accuracy: 0.9573 - fbeta_score: 0.8726 - val_loss: 2.9777 - val_accuracy: 0.9332 - val_fbeta_score: 0.7975\n",
            "Epoch 12/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 1.1508 - accuracy: 0.9621 - fbeta_score: 0.8848 - val_loss: 2.8044 - val_accuracy: 0.9341 - val_fbeta_score: 0.7908\n",
            "Epoch 13/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 1.1503 - accuracy: 0.9601 - fbeta_score: 0.8815 - val_loss: 2.7313 - val_accuracy: 0.9386 - val_fbeta_score: 0.7952\n",
            "Epoch 14/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.9771 - accuracy: 0.9667 - fbeta_score: 0.8987 - val_loss: 2.7922 - val_accuracy: 0.9332 - val_fbeta_score: 0.8078\n",
            "Epoch 15/100\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.9046 - accuracy: 0.9700 - fbeta_score: 0.9085 - val_loss: 3.1208 - val_accuracy: 0.9323 - val_fbeta_score: 0.8066\n",
            "Epoch 16/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.8497 - accuracy: 0.9694 - fbeta_score: 0.9058 - val_loss: 2.9272 - val_accuracy: 0.9368 - val_fbeta_score: 0.7844\n",
            "Epoch 17/100\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.7617 - accuracy: 0.9709 - fbeta_score: 0.9071 - val_loss: 2.9192 - val_accuracy: 0.9297 - val_fbeta_score: 0.8188\n",
            "Epoch 18/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.7072 - accuracy: 0.9727 - fbeta_score: 0.9125 - val_loss: 3.1391 - val_accuracy: 0.9332 - val_fbeta_score: 0.8111\n",
            "Epoch 19/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 0.7293 - accuracy: 0.9708 - fbeta_score: 0.9132 - val_loss: 3.1933 - val_accuracy: 0.9341 - val_fbeta_score: 0.8045\n",
            "Epoch 20/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 0.6855 - accuracy: 0.9727 - fbeta_score: 0.9173 - val_loss: 3.4684 - val_accuracy: 0.9314 - val_fbeta_score: 0.7968\n",
            "Epoch 21/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 0.6718 - accuracy: 0.9769 - fbeta_score: 0.9237 - val_loss: 3.1047 - val_accuracy: 0.9297 - val_fbeta_score: 0.7934\n",
            "Epoch 22/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.5945 - accuracy: 0.9745 - fbeta_score: 0.9347 - val_loss: 2.9886 - val_accuracy: 0.9359 - val_fbeta_score: 0.7909\n",
            "Epoch 23/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.4911 - accuracy: 0.9789 - fbeta_score: 0.9350 - val_loss: 3.0648 - val_accuracy: 0.9359 - val_fbeta_score: 0.8011\n",
            "Epoch 24/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.5469 - accuracy: 0.9778 - fbeta_score: 0.9270 - val_loss: 3.4992 - val_accuracy: 0.9341 - val_fbeta_score: 0.8034\n",
            "Epoch 25/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.4988 - accuracy: 0.9783 - fbeta_score: 0.9360 - val_loss: 3.1361 - val_accuracy: 0.9368 - val_fbeta_score: 0.8145\n",
            "Epoch 26/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.4449 - accuracy: 0.9815 - fbeta_score: 0.9455 - val_loss: 3.3930 - val_accuracy: 0.9350 - val_fbeta_score: 0.8023\n",
            "Epoch 27/100\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.4696 - accuracy: 0.9813 - fbeta_score: 0.9387 - val_loss: 3.4598 - val_accuracy: 0.9314 - val_fbeta_score: 0.7979\n",
            "Epoch 28/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.4816 - accuracy: 0.9804 - fbeta_score: 0.9416 - val_loss: 3.2623 - val_accuracy: 0.9377 - val_fbeta_score: 0.7911\n",
            "Epoch 29/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.4012 - accuracy: 0.9826 - fbeta_score: 0.9440 - val_loss: 3.2795 - val_accuracy: 0.9386 - val_fbeta_score: 0.7962\n",
            "Epoch 30/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 0.3708 - accuracy: 0.9830 - fbeta_score: 0.9476 - val_loss: 3.7803 - val_accuracy: 0.9270 - val_fbeta_score: 0.7903\n",
            "Epoch 31/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 0.4190 - accuracy: 0.9841 - fbeta_score: 0.9508 - val_loss: 3.0875 - val_accuracy: 0.9368 - val_fbeta_score: 0.8168\n",
            "Epoch 32/100\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.4352 - accuracy: 0.9804 - fbeta_score: 0.9490 - val_loss: 3.2640 - val_accuracy: 0.9386 - val_fbeta_score: 0.8156\n",
            "Epoch 33/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 0.3845 - accuracy: 0.9851 - fbeta_score: 0.9558 - val_loss: 3.4334 - val_accuracy: 0.9350 - val_fbeta_score: 0.7921\n",
            "Epoch 34/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 0.2976 - accuracy: 0.9849 - fbeta_score: 0.9543 - val_loss: 3.5323 - val_accuracy: 0.9332 - val_fbeta_score: 0.8222\n",
            "Epoch 35/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.3510 - accuracy: 0.9833 - fbeta_score: 0.9573 - val_loss: 3.2659 - val_accuracy: 0.9368 - val_fbeta_score: 0.8067\n",
            "Epoch 36/100\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.3066 - accuracy: 0.9857 - fbeta_score: 0.9496 - val_loss: 3.6418 - val_accuracy: 0.9332 - val_fbeta_score: 0.8033\n",
            "Epoch 37/100\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.2932 - accuracy: 0.9887 - fbeta_score: 0.9556 - val_loss: 3.7213 - val_accuracy: 0.9341 - val_fbeta_score: 0.8056\n",
            "Epoch 38/100\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.2840 - accuracy: 0.9876 - fbeta_score: 0.9572 - val_loss: 3.3838 - val_accuracy: 0.9305 - val_fbeta_score: 0.8055\n",
            "Epoch 39/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 0.3280 - accuracy: 0.9855 - fbeta_score: 0.9505 - val_loss: 3.5132 - val_accuracy: 0.9341 - val_fbeta_score: 0.8134\n",
            "Epoch 40/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.2532 - accuracy: 0.9898 - fbeta_score: 0.9667 - val_loss: 3.5526 - val_accuracy: 0.9341 - val_fbeta_score: 0.8033\n",
            "Epoch 41/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 0.2572 - accuracy: 0.9888 - fbeta_score: 0.9592 - val_loss: 3.3838 - val_accuracy: 0.9394 - val_fbeta_score: 0.8032\n",
            "Epoch 42/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.2326 - accuracy: 0.9910 - fbeta_score: 0.9616 - val_loss: 3.4939 - val_accuracy: 0.9323 - val_fbeta_score: 0.8100\n",
            "Epoch 43/100\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.2211 - accuracy: 0.9890 - fbeta_score: 0.9596 - val_loss: 3.9247 - val_accuracy: 0.9288 - val_fbeta_score: 0.8023\n",
            "Epoch 44/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.2495 - accuracy: 0.9871 - fbeta_score: 0.9597 - val_loss: 3.4639 - val_accuracy: 0.9350 - val_fbeta_score: 0.8065\n",
            "Epoch 45/100\n",
            "199/199 [==============================] - 4s 23ms/step - loss: 0.2603 - accuracy: 0.9895 - fbeta_score: 0.9603 - val_loss: 3.3079 - val_accuracy: 0.9332 - val_fbeta_score: 0.8180\n",
            "Epoch 46/100\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.2421 - accuracy: 0.9888 - fbeta_score: 0.9710 - val_loss: 4.0184 - val_accuracy: 0.9288 - val_fbeta_score: 0.8156\n",
            "\n",
            "Building suicide detector...\n",
            "Shuffling training data...\n",
            "Tokenizing corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████| 1788/1788 [00:01<00:00, 1496.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating term frequencies...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████| 1788/1788 [00:01<00:00, 1271.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating inverted document frequencies...\n",
            "Calculating tf-idf scores...\n",
            "Training neural network...\n",
            "Epoch 1/100\n",
            "48/48 [==============================] - 1s 11ms/step - loss: 24.4734 - accuracy: 0.5313 - fbeta_score: 0.5111 - val_loss: 17.1599 - val_accuracy: 0.5911 - val_fbeta_score: 0.6221\n",
            "Epoch 2/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 16.0261 - accuracy: 0.6340 - fbeta_score: 0.6020 - val_loss: 12.6469 - val_accuracy: 0.6506 - val_fbeta_score: 0.6869\n",
            "Epoch 3/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 12.4577 - accuracy: 0.6939 - fbeta_score: 0.6591 - val_loss: 10.6127 - val_accuracy: 0.7063 - val_fbeta_score: 0.7210\n",
            "Epoch 4/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 10.8375 - accuracy: 0.7294 - fbeta_score: 0.6825 - val_loss: 9.5144 - val_accuracy: 0.7509 - val_fbeta_score: 0.7197\n",
            "Epoch 5/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 9.4372 - accuracy: 0.7531 - fbeta_score: 0.7160 - val_loss: 8.7645 - val_accuracy: 0.7621 - val_fbeta_score: 0.7451\n",
            "Epoch 6/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 7.9258 - accuracy: 0.7841 - fbeta_score: 0.7380 - val_loss: 8.1958 - val_accuracy: 0.7918 - val_fbeta_score: 0.7689\n",
            "Epoch 7/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 7.6315 - accuracy: 0.7986 - fbeta_score: 0.7405 - val_loss: 8.0110 - val_accuracy: 0.8141 - val_fbeta_score: 0.7717\n",
            "Epoch 8/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 6.7223 - accuracy: 0.8104 - fbeta_score: 0.7810 - val_loss: 7.7204 - val_accuracy: 0.8253 - val_fbeta_score: 0.7940\n",
            "Epoch 9/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 6.3337 - accuracy: 0.8196 - fbeta_score: 0.7816 - val_loss: 7.4397 - val_accuracy: 0.8253 - val_fbeta_score: 0.7905\n",
            "Epoch 10/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 5.5459 - accuracy: 0.8354 - fbeta_score: 0.8080 - val_loss: 7.2060 - val_accuracy: 0.8364 - val_fbeta_score: 0.8111\n",
            "Epoch 11/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 5.3478 - accuracy: 0.8400 - fbeta_score: 0.8015 - val_loss: 7.0383 - val_accuracy: 0.8364 - val_fbeta_score: 0.8163\n",
            "Epoch 12/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 5.1432 - accuracy: 0.8466 - fbeta_score: 0.8143 - val_loss: 6.8829 - val_accuracy: 0.8364 - val_fbeta_score: 0.8232\n",
            "Epoch 13/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 4.5892 - accuracy: 0.8578 - fbeta_score: 0.8256 - val_loss: 6.6339 - val_accuracy: 0.8364 - val_fbeta_score: 0.8352\n",
            "Epoch 14/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 4.2951 - accuracy: 0.8552 - fbeta_score: 0.8227 - val_loss: 6.5887 - val_accuracy: 0.8327 - val_fbeta_score: 0.8268\n",
            "Epoch 15/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 3.8357 - accuracy: 0.8697 - fbeta_score: 0.8352 - val_loss: 6.5959 - val_accuracy: 0.8327 - val_fbeta_score: 0.8285\n",
            "Epoch 16/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 3.7517 - accuracy: 0.8729 - fbeta_score: 0.8454 - val_loss: 6.5077 - val_accuracy: 0.8364 - val_fbeta_score: 0.8542\n",
            "Epoch 17/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 3.6762 - accuracy: 0.8769 - fbeta_score: 0.8532 - val_loss: 6.2822 - val_accuracy: 0.8550 - val_fbeta_score: 0.8574\n",
            "Epoch 18/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 3.5735 - accuracy: 0.8848 - fbeta_score: 0.8562 - val_loss: 6.3395 - val_accuracy: 0.8476 - val_fbeta_score: 0.8597\n",
            "Epoch 19/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 3.2872 - accuracy: 0.8920 - fbeta_score: 0.8634 - val_loss: 6.1950 - val_accuracy: 0.8476 - val_fbeta_score: 0.8684\n",
            "Epoch 20/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 3.2169 - accuracy: 0.8861 - fbeta_score: 0.8558 - val_loss: 6.3357 - val_accuracy: 0.8401 - val_fbeta_score: 0.8616\n",
            "Epoch 21/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 2.8128 - accuracy: 0.8999 - fbeta_score: 0.8636 - val_loss: 6.2368 - val_accuracy: 0.8476 - val_fbeta_score: 0.8715\n",
            "Epoch 22/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 2.8213 - accuracy: 0.8960 - fbeta_score: 0.8699 - val_loss: 6.2279 - val_accuracy: 0.8513 - val_fbeta_score: 0.8547\n",
            "Epoch 23/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 2.7088 - accuracy: 0.8894 - fbeta_score: 0.8720 - val_loss: 6.1918 - val_accuracy: 0.8550 - val_fbeta_score: 0.8703\n",
            "Epoch 24/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 2.5500 - accuracy: 0.9006 - fbeta_score: 0.8685 - val_loss: 6.1170 - val_accuracy: 0.8476 - val_fbeta_score: 0.8734\n",
            "Epoch 25/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 2.0339 - accuracy: 0.9210 - fbeta_score: 0.8939 - val_loss: 6.0348 - val_accuracy: 0.8550 - val_fbeta_score: 0.8715\n",
            "Epoch 26/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 2.0375 - accuracy: 0.9171 - fbeta_score: 0.8894 - val_loss: 6.2297 - val_accuracy: 0.8476 - val_fbeta_score: 0.8820\n",
            "Epoch 27/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 2.0250 - accuracy: 0.9190 - fbeta_score: 0.8935 - val_loss: 6.0879 - val_accuracy: 0.8625 - val_fbeta_score: 0.8703\n",
            "Epoch 28/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 1.9449 - accuracy: 0.9256 - fbeta_score: 0.8954 - val_loss: 6.0538 - val_accuracy: 0.8662 - val_fbeta_score: 0.8734\n",
            "Epoch 29/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 1.8008 - accuracy: 0.9315 - fbeta_score: 0.9024 - val_loss: 6.1336 - val_accuracy: 0.8587 - val_fbeta_score: 0.8869\n",
            "Epoch 30/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 1.4633 - accuracy: 0.9322 - fbeta_score: 0.8977 - val_loss: 6.0952 - val_accuracy: 0.8587 - val_fbeta_score: 0.8858\n",
            "Epoch 31/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 1.6226 - accuracy: 0.9355 - fbeta_score: 0.9049 - val_loss: 6.0520 - val_accuracy: 0.8625 - val_fbeta_score: 0.8809\n",
            "Epoch 32/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 1.6164 - accuracy: 0.9315 - fbeta_score: 0.9058 - val_loss: 6.0992 - val_accuracy: 0.8736 - val_fbeta_score: 0.8820\n",
            "Epoch 33/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 1.4521 - accuracy: 0.9381 - fbeta_score: 0.9113 - val_loss: 6.1411 - val_accuracy: 0.8699 - val_fbeta_score: 0.8809\n",
            "Epoch 34/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 1.6226 - accuracy: 0.9302 - fbeta_score: 0.9175 - val_loss: 6.0574 - val_accuracy: 0.8662 - val_fbeta_score: 0.8827\n",
            "Epoch 35/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 1.1247 - accuracy: 0.9440 - fbeta_score: 0.9239 - val_loss: 6.0878 - val_accuracy: 0.8625 - val_fbeta_score: 0.8709\n",
            "Epoch 36/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 1.2041 - accuracy: 0.9447 - fbeta_score: 0.9233 - val_loss: 6.0863 - val_accuracy: 0.8662 - val_fbeta_score: 0.8809\n",
            "Epoch 37/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 1.2347 - accuracy: 0.9500 - fbeta_score: 0.9234 - val_loss: 5.9731 - val_accuracy: 0.8699 - val_fbeta_score: 0.8934\n",
            "Epoch 38/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.9447 - accuracy: 0.9579 - fbeta_score: 0.9248 - val_loss: 6.0507 - val_accuracy: 0.8662 - val_fbeta_score: 0.8820\n",
            "Epoch 39/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 1.0054 - accuracy: 0.9480 - fbeta_score: 0.9254 - val_loss: 6.0304 - val_accuracy: 0.8625 - val_fbeta_score: 0.8858\n",
            "Epoch 40/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 1.2151 - accuracy: 0.9500 - fbeta_score: 0.9262 - val_loss: 6.0184 - val_accuracy: 0.8662 - val_fbeta_score: 0.8925\n",
            "Epoch 41/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 1.0687 - accuracy: 0.9552 - fbeta_score: 0.9234 - val_loss: 6.0730 - val_accuracy: 0.8662 - val_fbeta_score: 0.8945\n",
            "Epoch 42/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.9198 - accuracy: 0.9546 - fbeta_score: 0.9372 - val_loss: 5.9794 - val_accuracy: 0.8662 - val_fbeta_score: 0.8896\n",
            "Epoch 43/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.8928 - accuracy: 0.9598 - fbeta_score: 0.9280 - val_loss: 5.9866 - val_accuracy: 0.8699 - val_fbeta_score: 0.8896\n",
            "Epoch 44/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.9120 - accuracy: 0.9579 - fbeta_score: 0.9287 - val_loss: 5.9029 - val_accuracy: 0.8587 - val_fbeta_score: 0.8797\n",
            "Epoch 45/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.8340 - accuracy: 0.9618 - fbeta_score: 0.9274 - val_loss: 5.9895 - val_accuracy: 0.8736 - val_fbeta_score: 0.8934\n",
            "Epoch 46/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.8250 - accuracy: 0.9526 - fbeta_score: 0.9405 - val_loss: 5.9300 - val_accuracy: 0.8662 - val_fbeta_score: 0.8885\n",
            "Epoch 47/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.7533 - accuracy: 0.9658 - fbeta_score: 0.9429 - val_loss: 5.9957 - val_accuracy: 0.8662 - val_fbeta_score: 0.8915\n",
            "Epoch 48/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.7642 - accuracy: 0.9645 - fbeta_score: 0.9441 - val_loss: 6.1062 - val_accuracy: 0.8699 - val_fbeta_score: 0.8839\n",
            "Epoch 49/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.7151 - accuracy: 0.9625 - fbeta_score: 0.9398 - val_loss: 6.0444 - val_accuracy: 0.8587 - val_fbeta_score: 0.8797\n",
            "Epoch 50/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.7640 - accuracy: 0.9697 - fbeta_score: 0.9493 - val_loss: 6.1188 - val_accuracy: 0.8625 - val_fbeta_score: 0.8983\n",
            "Epoch 51/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.8347 - accuracy: 0.9585 - fbeta_score: 0.9440 - val_loss: 6.0120 - val_accuracy: 0.8587 - val_fbeta_score: 0.8934\n",
            "Epoch 52/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.9664 - fbeta_score: 0.9458 - val_loss: 6.1068 - val_accuracy: 0.8550 - val_fbeta_score: 0.8847\n",
            "Epoch 53/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.6704 - accuracy: 0.9638 - fbeta_score: 0.9566 - val_loss: 6.0173 - val_accuracy: 0.8699 - val_fbeta_score: 0.8747\n",
            "Epoch 54/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5631 - accuracy: 0.9776 - fbeta_score: 0.9568 - val_loss: 5.9131 - val_accuracy: 0.8587 - val_fbeta_score: 0.8847\n",
            "Epoch 55/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.6713 - accuracy: 0.9717 - fbeta_score: 0.9482 - val_loss: 6.0991 - val_accuracy: 0.8550 - val_fbeta_score: 0.8847\n",
            "Epoch 56/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.5760 - accuracy: 0.9743 - fbeta_score: 0.9442 - val_loss: 5.9813 - val_accuracy: 0.8699 - val_fbeta_score: 0.8983\n",
            "Epoch 57/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5910 - accuracy: 0.9737 - fbeta_score: 0.9504 - val_loss: 5.9727 - val_accuracy: 0.8662 - val_fbeta_score: 0.8847\n",
            "Epoch 58/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.5912 - accuracy: 0.9730 - fbeta_score: 0.9628 - val_loss: 5.9247 - val_accuracy: 0.8662 - val_fbeta_score: 0.8983\n",
            "Epoch 59/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.6230 - accuracy: 0.9763 - fbeta_score: 0.9526 - val_loss: 5.9771 - val_accuracy: 0.8736 - val_fbeta_score: 0.8983\n",
            "Epoch 60/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.4342 - accuracy: 0.9776 - fbeta_score: 0.9559 - val_loss: 5.9623 - val_accuracy: 0.8736 - val_fbeta_score: 0.8934\n",
            "Epoch 61/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.5612 - accuracy: 0.9783 - fbeta_score: 0.9588 - val_loss: 5.9859 - val_accuracy: 0.8773 - val_fbeta_score: 0.8847\n",
            "Epoch 62/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5039 - accuracy: 0.9809 - fbeta_score: 0.9598 - val_loss: 5.9292 - val_accuracy: 0.8773 - val_fbeta_score: 0.8925\n",
            "Epoch 63/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.5830 - accuracy: 0.9737 - fbeta_score: 0.9562 - val_loss: 6.0042 - val_accuracy: 0.8699 - val_fbeta_score: 0.8945\n",
            "Epoch 64/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.5931 - accuracy: 0.9756 - fbeta_score: 0.9607 - val_loss: 5.9402 - val_accuracy: 0.8810 - val_fbeta_score: 0.8896\n",
            "Epoch 65/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5116 - accuracy: 0.9763 - fbeta_score: 0.9612 - val_loss: 6.0444 - val_accuracy: 0.8773 - val_fbeta_score: 0.8906\n",
            "Epoch 66/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.4697 - accuracy: 0.9849 - fbeta_score: 0.9650 - val_loss: 5.9952 - val_accuracy: 0.8699 - val_fbeta_score: 0.8925\n",
            "Epoch 67/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.6095 - accuracy: 0.9770 - fbeta_score: 0.9626 - val_loss: 5.9580 - val_accuracy: 0.8773 - val_fbeta_score: 0.8915\n",
            "Epoch 68/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.4613 - accuracy: 0.9796 - fbeta_score: 0.9642 - val_loss: 6.0371 - val_accuracy: 0.8810 - val_fbeta_score: 0.8983\n",
            "Epoch 69/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.4190 - accuracy: 0.9862 - fbeta_score: 0.9601 - val_loss: 6.0431 - val_accuracy: 0.8810 - val_fbeta_score: 0.8945\n",
            "Epoch 70/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.4586 - accuracy: 0.9835 - fbeta_score: 0.9633 - val_loss: 5.9973 - val_accuracy: 0.8736 - val_fbeta_score: 0.8953\n",
            "Epoch 71/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.4546 - accuracy: 0.9789 - fbeta_score: 0.9636 - val_loss: 6.0474 - val_accuracy: 0.8810 - val_fbeta_score: 0.8896\n",
            "Epoch 72/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.4521 - accuracy: 0.9849 - fbeta_score: 0.9633 - val_loss: 6.0719 - val_accuracy: 0.8810 - val_fbeta_score: 0.8866\n",
            "Epoch 73/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.4430 - accuracy: 0.9829 - fbeta_score: 0.9678 - val_loss: 6.1181 - val_accuracy: 0.8736 - val_fbeta_score: 0.9002\n",
            "Epoch 74/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.3931 - accuracy: 0.9868 - fbeta_score: 0.9651 - val_loss: 6.0456 - val_accuracy: 0.8773 - val_fbeta_score: 0.9002\n",
            "Epoch 75/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.4139 - accuracy: 0.9842 - fbeta_score: 0.9709 - val_loss: 6.1020 - val_accuracy: 0.8625 - val_fbeta_score: 0.8866\n",
            "\n"
          ]
        }
      ],
      "source": [
        "detectors = {}  # maps condition to detector object\n",
        "for condition in conditions:\n",
        "  print(\"Building\", condition, \"detector...\")\n",
        "  detectors[condition] = DetectionModel()\n",
        "  detectors[condition].train(*condition_data[condition])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "ibo1nj_Sq8_4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mtQRf7kS-J8"
      },
      "outputs": [],
      "source": [
        "def flag( sentence,\n",
        "          threshold = np.full(len(conditions), .5),\n",
        "          find_red_flags = False ):\n",
        "  '''\n",
        "  Inputs:\n",
        "    sentence: text we want to analyze for red flags\n",
        "    threshold: cosine similarity value above which a red flag is assigned to the sentence, for each condition\n",
        "      a higher threshold favors precision, a lower threshold favors recall\n",
        "  Output:\n",
        "    dictionary that maps condition name (str) to (numerical score, None or substring with highest red flag score)\n",
        "  '''\n",
        "  results = {}\n",
        "  for i, condition in enumerate(conditions):\n",
        "    score = detectors[condition].score(sentence)\n",
        "    biggest_red_flag = None\n",
        "    if find_red_flags and score >= threshold[i]: # red flag level meets threshold\n",
        "      split_sentence = sentence.split()\n",
        "      max_ngram_score = 0\n",
        "      max_ngram = (0,0)  # the segment of words that are most concerning as (start index, end index + 1)\n",
        "      for n in range(2,7): # n grams of varying sizes\n",
        "        for ngram_start in range(0, len(split_sentence) + 1 - n):\n",
        "          ngram_score = detectors[condition].score(\" \".join(split_sentence[ngram_start:ngram_start+n]))\n",
        "          if ngram_score > max_ngram_score:\n",
        "            max_ngram_score = ngram_score\n",
        "            max_ngram = (ngram_start, ngram_start+n)\n",
        "      biggest_red_flag = (\"...\" if max_ngram[0] > 0 else \"\") + \" \".join(split_sentence[max_ngram[0]:max_ngram[1]]) + (\"...\" if max_ngram[1] < len(split_sentence) else \"\")\n",
        "\n",
        "    results[condition] = (score, biggest_red_flag)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ki_7u446BHMb",
        "outputId": "99128d19-f59e-43c8-8f63-47f94defcd79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'anxiety': (0.0, None),\n",
              " 'stress': (1.0835822195076616e-06, None),\n",
              " 'depression': (7.476426006202954e-34, None),\n",
              " 'suicide': (4.4346108859755925e-17, None)}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "flag(\"wow, I'm enjoying life. it's so much fun!\", find_red_flags = True)  # notably positive valence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMORtsCsAlSN",
        "outputId": "a8a460b4-0def-43f2-c92a-df4910550e67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'anxiety': (0.0, None),\n",
              " 'stress': (1.0866306672596723e-27, None),\n",
              " 'depression': (0.0, None),\n",
              " 'suicide': (9.635192412899177e-31, None)}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "flag(\"hey, do you wanna play video games later?\", find_red_flags = True)  # neutral/mild positive valence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wf9HMQNAv5n",
        "outputId": "5de32cd9-3412-47ed-831a-12ea0e47704e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'anxiety': (3.914957247275197e-09, None),\n",
              " 'stress': (1.0, '...feeling unwell lately'),\n",
              " 'depression': (1.0, '...been feeling...'),\n",
              " 'suicide': (0.011789314448833466, None)}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "flag(\"I have been feeling unwell lately\", find_red_flags = True)  # mild negative valence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnT5IZWiBHMc",
        "outputId": "841f77c8-8507-4aed-9688-f7ed08861f62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'anxiety': (0.0, None),\n",
              " 'stress': (1.0, '...hopeless, nothing...'),\n",
              " 'depression': (1.0, 'everything is...'),\n",
              " 'suicide': (1.0, '...life is...')}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "flag(\"everything is hopeless, nothing works and life is depressing\", find_red_flags = True)  # strongly negative valence"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N9-TK41U9oV3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}